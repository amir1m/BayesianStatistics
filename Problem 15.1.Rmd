---
title: "Problem 15.1"
output:
  pdf_document:
    latex_engine: xelatex
always_allow_html: true
---

# 15.1.1
Use your statistical software of choice generate 100 independent samples of (μt,μc). Draw a scatter plot of your (μt,μc) samples, with lines connecting consecutive points. How close are the sample-estimated means to the true means? (Hint: to do this in R you will need to use the MASS package:

```{r}
library (MASS)
library (mvtnorm)
Sigma <- matrix(c(2,0.8,0.8,0.5),2,2)
hist(mvrnorm(n = 100, c(20,5), Sigma), main = "Mean of mu_t and mu_c")
```

# 15.1.2
Code up a Random Walk Metropolis sampler for this example. This is composed of the following steps:

```{r}
ldebug <- 0
Sigma <- matrix(c(2, 0.8, 0.8, 0.5), 2, 2)
```

## 1 
Create a proposal function that takes a current value of θ = (μt, μc) and outputs a proposed value of these using a multivariate normal centred on the current estimates. (Here use a multivariate normal proposal with an identity covariance matrix.)

```{r}
fproposal <- function(theta_current){
  if (ldebug == 1) cat("\nfproposal:theta_current = ", theta_current)
  return(mvrnorm(n=1, mu = theta_current,Sigma = Sigma))
}
```


## 2
Create a function which takes as inputs θcurrent and θproposed, and outputs the ratio of the posteriors of the proposed value to the current one (Hint: to do this in R you will need to use the following to calculate the value of the posterior at (x, y):

```{r}

fGetRatio <- function(theta_current, theta_proposed) {
  r <- dmvnorm(theta_proposed, c(20, 5), sigma = Sigma) / dmvnorm(theta_current ,c(20, 5), sigma = Sigma)
  
  if (ldebug == 1) cat("\nfGetRatio:r= ", r)
  
  return(r)

}

```

## 3
Create an accept/reject function which takes as inputs θcurrent and θproposed, then uses the
above ratio function to find: r = θproposed ; then compares r with a uniformly-distributed θcurrent random number u between 0 and 1. If r > u =⇒ output θproposed; otherwise output θcurrent

```{r}
fAcceptReject <- function(theta_current, theta_proposed) {
  r <- fGetRatio(theta_current, theta_proposed)
  
  u <- runif(1,0,1)
  if ( r > u)
   return(theta_proposed)
  else
   return(theta_current)
}

```

## 4
Combine the proposal function along with the accept/reject function to make a function that takes as input θcurrent, proposes a new value of θ, then based on r moves to that new point or stays in the current position.

```{r}

fSample <- function(theta_current) {
  if (ldebug == 1) cat("\nfSample: theta_current= ", theta_current)
  
  theta_proposed <- fproposal(theta_current)
  if (ldebug == 1) cat("\nfSample:theta_proposed= ", theta_proposed)
  
  theta_accepted <- fAcceptReject(theta_current, theta_proposed)
  if (ldebug == 1) cat("\nfSample:theta_accepted= ", theta_accepted)
  
  return(theta_accepted)
  
}

```

## 5
Create a function called “RWMetropolis” that takes a starting value of θ and runs for n steps

```{r}
RWMetropolis <- function(theta, N) {
  if (ldebug == 1) cat("\nRWMetropolis: theta = ", theta)
  
  nRejected <- 0
  nextSample <- matrix(nrow = 1, ncol = 2)
  
  ltheta <- matrix(nrow = N, ncol = 2)
  ltheta[1,] <- theta
  
  for (t in 2:N){
    nextSample[1,] <- fSample(ltheta[t-1,])
  
    ifelse(nextSample[1,] == ltheta[t-1,], nRejected <- nRejected + 1, 0)
    
    ltheta[t,] <- nextSample[1,]
    
  }
  
  result <- list("nRejected" = nRejected , "samples" = ltheta)
  return(result)
}
```

# 15.1.2
```{r}
set.seed(42)
theta_starting <- c(10,5)
ldebug <- 0
result <- RWMetropolis(theta_starting,100)
noOfRejectedSamples <- result$"nRejected"
lRWMsamples <- result$samples

cat("\n No. of Rejected Samples: ", noOfRejectedSamples)
cat("\n % of rejected samples = ", noOfRejectedSamples/(100 + noOfRejectedSamples) )

```


```{r}
plot(lRWMsamples, type = 'l', xlab = "mu_t", ylab = "mu_c", main = "Random Wlak Metropolis")
```

```{r}
hist(lRWMsamples[,2], col ="blue", xlab = "mu_c")
hist(lRWMsamples[,1], col ="orange", xlab = "mu_t")
```

# 15.1.3
```{r}
acceptancePerc <- vector()
for(i in 1:100) {
  result <- RWMetropolis(theta_starting,100)
  acceptancePerc[i] <- result$"nRejected" / (100 + result$"nRejected")
}

hist(acceptancePerc, xlab = "rejection %", main = "RWM ARejection %", col = "blue")

```

# 15.1.4
```{r}
fWithin <- function(lSamples){
  return(mean(sapply(lSamples, var)))
}
## Testing it
#lSamples <- lapply(seq(1, 10, 1), function(i) rbinom(100, 100, 0.5))
#fWithin(lSamples)
```

```{r}
fBetween <- function(lSamples){
  lMean <- sapply(lSamples, mean)
  aMean <- mean(lMean)
  m <- length(lSamples)
  t <- length(lSamples[[1]])
  return((t / (m - 1)) * sum((lMean - aMean) ^ 2))
}
  
```

```{r}
fRhat <- function(lSamples){
  W <- fWithin(lSamples)
  B <- fBetween(lSamples)
  t <- length(lSamples[[1]])
  return(sqrt((W + (1 / t) * (B - W)) / W))
}
```


# 15.1.5
```{r}
theta_starting <- c(20,5)
numSamples <- 5

acceptancePerc <- vector()

lSamples <- list()

for(i in 1:8) {
  result <- RWMetropolis(theta_starting,numSamples)
  lRWMsamples <- result$samples
  lSamples[[i]] <- lRWMsamples
  acceptancePerc[i] <- result$"nRejected" / (numSamples + result$"nRejected")
}

lRhat <- fRhat(lSamples)
```

```{r}
cat("\nRhat : ", lRhat)
cat("\n Rejected % : ", acceptancePerc)
```

# 15.1.6
Using eight chains calculate Rˆ for each of (μt, μc) for a sample size of 100. This time make sure to start your chains in overdispersed positions in parameter space. Use a random number from a multivariate normal centred on the posterior means with a covariance matrix of 40 times the identity matrix

```{r}

numSamples <- 1000
acceptancePerc <- vector()
lSamples <- list()

for(i in 1:8) {
  theta_starting <- mvrnorm(n=1, c(20, 5), Sigma = 40 * diag(2))
  result <- RWMetropolis(theta_starting,numSamples)
  lRWMsamples <- result$samples
  lSamples[[i]] <- lRWMsamples
  acceptancePerc[i] <- result$"nRejected" / (numSamples + result$"nRejected")
}

lRhat <- fRhat(lSamples)

```


```{r}
cat("\nRhat : ", lRhat)
cat("\nRejected % : ", acceptancePerc)

```

```{r}
dim(lSamples[[1]])
```

# 15.1.7
After approximately how many iterations does Random Walk Metropolis reach Rˆ < 1.1?

```{r}

mu_t <- matrix(nrow = 8, ncol = numSamples)
mu_c <- matrix(nrow = 8, ncol = numSamples)

for (i in 1:8) {
  mu_t[i,] <- lSamples[[i]][,1]
  mu_c[i,] <- lSamples[[i]][,2]
}


```


```{r}
palette()
```


```{r}

plot(mu_t[1,], type="l", col=1, ylim = c(0, 35), xlab = "iteration#", 
     ylab = expression(paste(mu[t])), 
     main = expression(paste("Figure 15.5: The path taken by each chain in ", μ[t]," space")))

for(i in 2:8){
  lines(mu_t[i,], type="l", col=i)
}

```

```{r}
plot(mu_c[1,], type="l", col=1, ylim = c(0, 15), xlab = "iteration#", 
     ylab = expression(paste(mu[c])), 
     main = expression(paste("Figure 15.5 : The path taken by each chain in ", μ[c]," space")))

for(i in 2:8){
  lines(mu_c[i,], type="l", col=i)
}
```

# 15.1.8
The conditional distributions of each variable are given by:
μt ∼N(20+1.6(μc −5),(1−0.82)2) μc ∼N(5+0.4(μt −20),(1−0.82)0.5)
Use this information to code up a Gibbs sampler, again starting at (μt,μc) = (10,5). (Hint: in R use rnorm, or equivalent to create two functions: one that produces draws of μt given μc; and the other that produces draws of μc given μt. Then create a function that cycles between these updates. Make sure to always draw samples using the most recent values of (μt,μc)).

```{r}

fGetmu_t <- function(mu_c){
  result <- rnorm(n = 1, mean = (20 + 1.6*(mu_c - 5)), sd = (1 - 0.8^2)*2)
  
  return(result)
}
```

```{r}
fGetmu_c <- function(mu_t){
  result <- rnorm(n = 1, mean = (5 + 0.4 *(mu_t - 20)), sd = (1 - 0.8^2)*0.5)
  return(result)
}

```


```{r}
fGibbs <- function(theta_starting, numSamples) {
  lSamples = matrix(nrow = numSamples, ncol = 2)
  
  lSamples[1,] <- theta_starting
  
  for(i in 2:numSamples){
    mu_t_i <- fGetmu_t(lSamples[i-1,2])
    mu_c_i <- fGetmu_c(mu_t_i)
    lSamples[i,] <- c(mu_t_i, mu_c_i)
  }
  
  return(lSamples)
}

```


# 15.1.9. 
Use your Gibbs sampler to draw 100 samples. Draw a scatter plot of your (μt, μc) samples with lines connecting consecutive points. Discarding the first 50 observations, how do the estimates of the mean of each parameter compare with their true values?

```{r}
lGibbsSampleOriginal <- fGibbs(c(10,5), 100)
lGibbsSampleWarmed <- lGibbsSampleOriginal[51:100,]

```

```{r}
library(MASS)
x <- lGibbsSampleOriginal[,1]
y <- lGibbsSampleOriginal[,2]
plot(x,y, type = "l" )
contour(kde2d(x,y), add = T, drawlabels = F, col = "blue" )
```

```{r}
library(plotly)
x <- lGibbsSampleOriginal[,1]
y <- lGibbsSampleOriginal[,2]
s <- subplot(
  plot_ly(x = x, type = "histogram"),
  plotly_empty(),
  plot_ly(x = x, y = y, type = "histogram2dcontour"),
  plot_ly(y = y, type = "histogram"),
  nrows = 2, heights = c(0.2, 0.8), widths = c(0.8, 0.2), margin = 0,
  shareX = TRUE, shareY = TRUE, titleX = FALSE, titleY = FALSE
)
fig <- layout(s, showlegend = FALSE)

fig



```

```{r}
fig <- plot_ly(x = x, y=y) 

fig <- fig %>%
  add_trace(type='histogram2dcontour')

fig

```

```{r}
hist(lGibbsSampleOriginal, main = "Mean of mu_c and mu_t")
```

# 15.1.10 
Generate 200 samples from each of your Random Walk Metropolis and Gibbs samplers. Discard the first 100 observations of each as warm-up. For each calculate the error in estimating the posterior mean of μt. Repeat this exercise 40 times; each time recording the error. How does their error compare to the independent sampler?



```{r}
set.seed(42)
numSamples <- 200
numIterations <- 200

ltrue_mut <- 20
ltrue_muc <- 5

ltrue_sigma <- matrix(c(2,0.8,0.8,0.5),2,2)
ltrue_mu <-  c(20,5)

theta_starting <- c(10,5)

lerror_rwm <- matrix(nrow = numIterations, ncol = 2)

lerror_gibbs <- matrix(nrow = numIterations, ncol = 2)

lerror_indept <- matrix(nrow = numIterations, ncol = 2)

for(i in 1:numIterations) {
  lRWMsamples <- RWMetropolis(theta_starting,numSamples)$samples
  lRWMsamplesWarmed <- lRWMsamples[101:200,]
  err_mut <- mean(lRWMsamplesWarmed[, 1]) -  ltrue_mut
  err_muc <- mean(lRWMsamplesWarmed[, 2]) -  ltrue_muc
  lerror_rwm[i,] <- c(err_mut, err_muc)
  
  lGibbsSample <- fGibbs(theta_starting, numSamples)
  lGibbsSampleWarmed <- lGibbsSample[101:200,]
  err_mut <- mean(lGibbsSampleWarmed[, 1]) -  ltrue_mut
  err_muc <- mean(lGibbsSampleWarmed[, 2]) -  ltrue_muc
  lerror_gibbs[i,] <- c(err_mut, err_muc)
  
  lIndependentSamples <-  mvrnorm(n = 100, mu=ltrue_mu, Sigma = ltrue_sigma)
  err_mut <- mean(lIndependentSamples[, 1]) -  ltrue_mut
  err_muc <- mean(lIndependentSamples[, 2]) -  ltrue_muc
  lerror_indept[i,] <- c(err_mut, err_muc)
  
}

```

  
```{r}
hist(lerror_rwm[,1], xlim = c(-2,2), col = "orange", main="Figure 15.8:Error mu_t", xlab = "error")
hist(lerror_gibbs[,1], col = "green", add=T)
hist(lerror_indept[,1], col = "blue", add=T)

```

```{r}
cat("\nmu_t Error:")
cat("\n   RWM Error: ", sd(lerror_rwm[,1]))
cat("\n   Gibbs Error:", sd(lerror_gibbs[,1]))
cat("\n   Independent Error:", sd(lerror_indept[,1]))

```

#15.1.11. 
Repeat Problem 15.10 to obtain the average error in estimating the posterior mean of μt across arange of sample sizes n=5 to n=200.

```{r}

set.seed(42)
#numSamples <- 200
lMinNumSamples <- 5
lMaxNumSamples <- 200

numIterations <- 100

ltrue_mut <- 20
ltrue_muc <- 5

ltrue_sigma <- matrix(c(2,0.8,0.8,0.5),2,2)
ltrue_mu <-  c(20,5)

theta_starting <- c(10,5)


lerror_samples_rwm <- matrix(nrow = lMaxNumSamples - lMinNumSamples+1, ncol = 2 )
lerror_samples_gibbs <- matrix(nrow = lMaxNumSamples - lMinNumSamples+1, ncol = 2 )
lerror_samples_indep <- matrix(nrow = lMaxNumSamples - lMinNumSamples+1, ncol = 2 )

ldebug <- 0
for (numSamples in lMinNumSamples:lMaxNumSamples ){
  lSampleRange <- seq(from=round(numSamples/2) + 1, to=numSamples)
  lSampleIndex <- numSamples - lMinNumSamples + 1

  
  lerror_rwm <- matrix(nrow = numIterations, ncol = 2)
  lerror_gibbs <- matrix(nrow = numIterations, ncol = 2)
  lerror_indep <- matrix(nrow = numIterations, ncol = 2)

  for(i in 1:numIterations) {
    lRWMsamples <- RWMetropolis(theta_starting,numSamples)$samples
    lRWMsamplesWarmed <- lRWMsamples[lSampleRange,]
    err_mut <- mean(lRWMsamplesWarmed[, 1]) -  ltrue_mut
    err_muc <- mean(lRWMsamplesWarmed[, 2]) -  ltrue_muc
    lerror_rwm[i,] <- c(err_mut, err_muc)
    
    lGibbsSample <- fGibbs(theta_starting, numSamples)
    lGibbsSampleWarmed <- lGibbsSample[lSampleRange,]
    err_mut <- mean(lGibbsSampleWarmed[, 1]) -  ltrue_mut
    err_muc <- mean(lGibbsSampleWarmed[, 2]) -  ltrue_muc
    lerror_gibbs[i,] <- c(err_mut, err_muc)
    
    lIndependentSamples <-  mvrnorm(n = round(numSamples/2), mu=ltrue_mu, Sigma = ltrue_sigma)
    err_mut <- mean(lIndependentSamples[, 1]) -  ltrue_mut
    err_muc <- mean(lIndependentSamples[, 2]) -  ltrue_muc
    lerror_indep[i,] <- c(err_mut, err_muc)
  }
  
  lerror_samples_rwm[lSampleIndex,] <- c(sd(lerror_rwm[,1]), sd(lerror_rwm[,2]))
  lerror_samples_gibbs[lSampleIndex,] <- c(sd(lerror_gibbs[,1]), sd(lerror_gibbs[,2]))
  lerror_samples_indep[lSampleIndex,] <- c(sd(lerror_indep[,1]), sd(lerror_indep[,2]))
}

```

# 15.1.12
Using the results from the previous question estimate the effective sample size for 150 observations of the Random Walk Metropolis and Gibbs samplers.

```{r}

plot(lerror_samples_rwm[,1], col = "orange", type="l", ylim = c(0,3), yaxp=c(0,3,20), xaxp=c(0,200, 50),  xlab = "sample size", ylab = "mean error")
lines(lerror_samples_gibbs[,1], col="green")
lines(lerror_samples_indep[,1], col="blue")

abline(h=lerror_samples_rwm[,1][150], col= "orange", lty=2)
abline(v=8, col= "orange", lty=2) #ESS of RWM

abline(h=lerror_samples_gibbs[,1][150], col= "green", lty=2)
abline(v=60, col= "green", lty=2) #ESS of Gibbs

legend(150, 2.5, legend=c("independent", "RWM", "Gibbs"),
       col=c("blue", "orange", "green"), lty=1:1, cex=0.8)

```


# 15.1.14
Code up a Hamiltonian Monte Carlo sampler for this problem. (Alternatively, use the functions provided in the R file “HMC scripts.R” adapted from [9]). Use a standard deviation of the momentum proposal distribution (normal) of 0.18, along with a step size ε = 0.18 and L = 10 individual steps per iteration to simulate 100 samples from the posterior. How does the estimate of the mean compare with that from the Independent, Random Walk Metropolis and Gibbs samplers?


## Get the HMC routine code given by author
```{r}

## HMC for a single step
HMC <- function (current_q, U, grad_U, epsilon, L, aSigma){
  q = current_q
  p = rnorm(length(q),0,aSigma)  # independent standard normal variates
  current_p = p
  
  # Make a half step for momentum at the beginning
  
  p = p - epsilon * grad_U(q) / 2
  
  # Alternate full steps for position and momentum
  
  for (i in 1:L)
  {
    # Make a full step for the position
    
    q = q + epsilon * p
    
    # Make a full step for the momentum, except at end of trajectory
    
    if (i!=L) p = p - epsilon * grad_U(q)
  }
  
  # Make a half step for momentum at the end.
  
  p = p - epsilon * grad_U(q) / 2
  
  # Negate momentum at end of trajectory to make the proposal symmetric
  
  p = -p
  
  # Evaluate potential and kinetic energies at start and end of trajectory
  
  current_U = U(current_q)
  current_K = sum(current_p^2) / 2
  proposed_U = U(q)
  proposed_K = sum(p^2) / 2
  
  #   print(current_U-proposed_U)
  #   print(current_K-proposed_K)
  
  # Accept or reject the state at end of trajectory, returning either
  # the position at the end of the trajectory or the initial position
  r <- exp(current_U-proposed_U+current_K-proposed_K)
  # print(r)
  if (runif(1) < r)
  {
    return (q)  # accept
  }
  else
  {
    return (current_q)  # reject
  }
}

## Gradient of the potential with respect to x
fGradSimpleX <- function(x,y){
  aGrad <- -0.5 *exp(1/2 * (-(-20 + x) * (1.38889 * (-20 + x) - 
                                            2.22222 * (-5 + y)) - (-2.22222 * (-20 + x) + 
                                                                     5.55556 * (-5 + y)) * (-5 + y)) + 
                       1/2 * ((-20 + x) * (1.38889 * (-20 + x) - 
                                             2.22222 * (-5 + y)) + (-2.22222 * (-20 + x) + 
                                                                      5.55556 * (-5 + y)) * (-5 + y))) *(-2.77778 * (-20 + x) + 4.44444 * (-5 + y))
  return(aGrad)
}

## Gradient of the potential with respect to y
fGradSimpleY <- function(x,y){
  aGrad <- -0.5 * exp(1/2 * (-(-20 + x) * (1.38889 * (-20 + x) - 
                                             2.22222 * (-5 + y)) - (-2.22222 * (-20 + x) + 
                                                                      5.55556 * (-5 + y)) * (-5 + y)) + 
                        1/2 * ((-20 + x) * (1.38889 * (-20 + x) - 
                                              2.22222 * (-5 + y)) + (-2.22222 * (-20 + x) + 
                                                                       5.55556 * (-5 + y)) * (-5 + y))) * (4.44444 * (-20 + x) - 
                                                                                                             11.1111 * (-5 + y))
  
  return(aGrad)
}

## Gradient of U with respect to both coordinates
grad_U <- function(aQ){
  aGradX <- fGradSimpleX(aQ[[1]],aQ[[2]])
  aGrady <- fGradSimpleY(aQ[[1]],aQ[[2]])
  return(c(aGradX,aGrady))
}

## Potential function given by the -ve log of the posterior
U <- function(aQ){
  x <- aQ[[1]]
  y <- aQ[[2]]
  aU <- -log(0.265258 * exp(
    1/2 * (-(-20 + x) * (1.38889 * (-20 + x) - 
                       2.22222 * (-5 + y)) - (-2.22222 * (-20 + x) + 
                                              5.55556 * (-5 + y)) * (-5 + y))))
  return(aU) 
}

## Simulate a number of HMC steps 
fHMCAllSteps <- function(nIterations,start_q,U, grad_U, epsilon, L, aSigma){
  
  mSamples <- matrix(nrow=nIterations,ncol=2)
  mSamples[1,] <- start_q
  current_q <- start_q
  for (i in 1:nIterations){
    current_q <- HMC(current_q, U, grad_U, epsilon, L, aSigma)
    mSamples[i,] <- current_q
  }
  return(mSamples)
}

## Try out the function
lSamples <- as.data.frame(fHMCAllSteps(100,c(22,6),U,grad_U,0.18,10,0.18))
ggplot(lSamples,aes(x=V1,y=V2)) + geom_path()

mean(lSamples$V2)



# HMC_keep <- function (current_q, U, grad_U, epsilon, L, aSigma){
#   q = current_q
#   p = rnorm(length(q),0,aSigma)  # independent standard normal variates
#   current_p = p
#   
#   # Make a half step for momentum at the beginning
#   
#   p = p - epsilon * grad_U(q) / 2
#   
#   # Alternate full steps for position and momentum
#   
#   lPosition <- matrix(nrow = (L+1),ncol = 2)
#   lPosition[1,] <- q
#   
#   for (i in 1:L)
#   {
#     
#     # Make a full step for the position
#     
#     q = q + epsilon * p
#     lPosition[(i+1),] <- q
#     
#     # Make a full step for the momentum, except at end of trajectory
#     
#     if (i!=L) p = p - epsilon * grad_U(q)
#   }
#   
#   # Make a half step for momentum at the end.
#   
#   p = p - epsilon * grad_U(q) / 2
#   
#   # Negate momentum at end of trajectory to make the proposal symmetric
#   
#   p = -p
#   
#   # Evaluate potential and kinetic energies at start and end of trajectory
#   
#   current_U = U(current_q)
#   current_K = sum(current_p^2) / 2
#   proposed_U = U(q)
#   proposed_K = sum(p^2) / 2
#   
#   #   print(current_U-proposed_U)
#   #   print(current_K-proposed_K)
#   
#   # Accept or reject the state at end of trajectory, returning either
#   # the position at the end of the trajectory or the initial position
#   r <- exp(current_U-proposed_U+current_K-proposed_K)
#   # print(r)
#   if (runif(1) < 5)
#   {
#     return (list(q=q,pos=lPosition))  # accept
#   }
#   else
#   {
#     return (list(q=current_q,pos=lPosition))  # reject
#   }
# }
# 
# nReplicates <- 100
# nStep <- 100
# mAll <- matrix(ncol = nReplicates,nrow = nStep)
# for(i in 1:nReplicates){
#   lTest <- HMC_keep(c(20,5), U, grad_U, 0.18, nStep, 0.18)
#   lTemp <- lTest$pos[,1]
#   aLen <- length(lTemp)
#   mAll[,i] <- lTemp[1:(aLen-1)]
# }
# 
# library(reshape2)
# mAll <- melt(mAll)
# library(ggplot2)
# ggplot(mAll,aes(x=Var1,colour=as.factor(Var2),y=value)) + geom_path() + theme(legend.position="none") +
#   ylab('mu_t') + xlab('number of steps')
# 
# 
# par(mfrow=c(1,4))
# lTest <- HMC_keep(c(20,5), U, grad_U, 0.18, 10, 0.18)
# plot(lTest$pos,type='l',xlab='mu_t',ylab='mu_c',main='L=10') 
# lTest <- HMC_keep(c(20,5), U, grad_U, 0.18, 20, 0.18)
# plot(lTest$pos,type='l',xlab='mu_t',ylab='mu_c',main='L=20') 
# lTest <- HMC_keep(c(20,5), U, grad_U, 0.18, 50, 0.18)
# plot(lTest$pos,type='l',xlab='mu_t',ylab='mu_c',main='L=50') 
# lTest <- HMC_keep(c(20,5), U, grad_U, 0.18, 100, 0.18)
# plot(lTest$pos,type='l',xlab='mu_t',ylab='mu_c',main='L=100') 

```


## HMC Solution
```{r}
set.seed(42)
#numSamples <- 200
lMinNumSamples <- 5
lMaxNumSamples <- 200

numIterations <- 100

ltrue_mut <- 20
ltrue_muc <- 5

ltrue_sigma <- matrix(c(2,0.8,0.8,0.5),2,2)
ltrue_mu <-  c(20,5)

theta_starting <- c(10,5)


lerror_samples_hmc <- matrix(nrow = lMaxNumSamples - lMinNumSamples+1, ncol = 2 )

ldebug <- 0

for (numSamples in lMinNumSamples:lMaxNumSamples ){
  lSampleRange <- seq(from=round(numSamples/2) + 1, to=numSamples)
  lSampleIndex <- numSamples - lMinNumSamples + 1

  
  lerror_hmc <- matrix(nrow = numIterations, ncol = 2)

  for(i in 1:numIterations) {
    lHMCsamples <- fHMCAllSteps(numSamples, theta_starting, U, grad_U, 0.18, 10, 0.18) 
    lHMCsamplesWarmed <- lHMCsamples[lSampleRange,]
    err_mut <- mean(lHMCsamplesWarmed[, 1]) -  ltrue_mut
    err_muc <- mean(lHMCsamplesWarmed[, 2]) -  ltrue_muc
    lerror_hmc[i,] <- c(err_mut, err_muc)
  }
  
  lerror_samples_hmc[lSampleIndex,] <- c(sd(lerror_hmc[,1]), sd(lerror_hmc[,2]))
 
}


```


```{r}
plot(lerror_samples_rwm[,1], col = "orange", type="l", ylim = c(0,3),  xaxp=c(0,200, 50), yaxp=c(0,3,20), xlab = "sample size", ylab = "mean error")
lines(lerror_samples_gibbs[,1], col="green")
lines(lerror_samples_indep[,1], col="blue")
lines(lerror_samples_hmc[,1], col="purple")

legend(150, 2.5, legend=c("independent", "RWM", "Gibbs", "HMC"),
       col=c("blue", "orange", "green", "purple"), lty=1:1, cex=0.8)

abline(h=0)
```

# 15.1.17 
You receive new data that results in a change in the posterior to: 

Using your Random Walk Metropolis sampler calculate Rˆ for 8 chains; each generating 100 samples for each.

```{r}
set.seed(42)
#numSamples <- 100
acceptancePerc <- vector()


mu <- c(20,5)
sigma <- matrix(c(2,0.99,0.99,0.5),2,2)


fRunChain_RWM <- function(numChain, numSamples){
  for(i in 1:numChain) {
      theta_starting <- mvrnorm(n=1, mu = mu, Sigma = sigma)
      lRWMsamplesWarmed <- RWMetropolis(theta_starting,numSamples)$samples[seq(from=round(numSamples/2) + 1, to=numSamples), ]
      lSamples[[i]] <- lRWMsamplesWarmed
    }
  return(fRhat(lSamples))
}

fRunChain_RWM_SampleRange <- function(lSampleRange){
  #cat("\nlSampleRange: ", lSampleRange)
  lRhat <- vector()
  i <- 1
  for (numSamples in lSampleRange) {
    lRhat[i] <- fRunChain_RWM(8, numSamples)
    i <- i + 1
  }
  
  return(lRhat)
}

rh <- replicate(4,fRunChain_RWM_SampleRange(seq(5,200)))


```

```{r}
cat("\nRhat : ", dim(rh),"\n")

```

```{r}
plot(rh[,1], type="l", col=1)


for (i in seq(2:4)) {
  lines(rh[,i], col=i)
}
```


## TODO : Parallelize it!



