---
title: "Problem 14.1"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---


# Problem 14.1.2
Assuming priors of the form: $\pi ∼ beta(α_ψ,β_π), S ∼ beta(α_S,β_S)$ and $C ∼ beta(α_C,β_C)$, it is possible to code up a Gibbs sampler for this problem [8] of the form 

\centering
![equations](images/Equations_Problem_14_1_2.png)


```{r}
fGibbsSampling <- function(numSamples,a, b){
  
  # cat("\nBefore Loop: ")
  # cat("\nY1:",Y1, " Y2: ", Y1, " lp: ", lpi, " S: ", S, " C: ", C)
  for(t in 2:numSamples){
    
    Y1[t] <- rbinom(n = 1, size = a, prob =  lpi[t-1] * S[t-1] / 
                      (lpi[t-1] * S[t-1] + (1 - lpi[t-1]) * (1 - C[t-1]) ))
    
    Y2[t] <- rbinom(n=1, size = b, prob = lpi[t-1] * (1 - S[t-1]) / 
                      (lpi[t-1] * (1 - S[t-1]) + (1 - lpi[t-1]) * C[t-1]))
    
    lpi[t] <- rbeta(1, shY1[t] + Y2[t] + alpha_pi, a + b - Y1[t] - 
                      Y2[t] + beta_pi)
    
    S[t] <- rbeta(1, Y1[t] + alpha_S, Y2[t] + beta_S)
    
    C[t] <- rbeta(1, b - Y2[t] + alpha_C, a - Y1[t] + beta_C)
    # cat("\n\nAfter t=", t)
    # cat("\nY1:",Y1, " Y2: ", Y1, " lp: ", lpi, " S: ", S, " C: ", C)
    
  }
  
  hist(lpi)
  hist(Y1)
  hist(Y2)
  hist(S)
  hist(C)
  
}
  
  
```


# Problem 14.1.3
Suppose that out of a sample of 100 people, 20 of those tested negative and 80 positive. Assuming uniform priors on π, S and C, use Gibbs sampling to generate posterior samples for π. What do you conclude?

```{r}
set.seed(41)
Y1 <- vector()
Y2 <- vector()
lpi <- vector()
S <- vector()
C <- vector()

a <- 80
b <- 20

Y1[1] <- as.integer(runif(1, 1, a))
  
Y2[1] <- as.integer(runif(1, a, b+a))
  
lpi[1] <- runif(1, 0, 1)
  
S[1] <- runif(1, 0, 1)
  
C[1] <- runif(1, 0, 1)

# Priors
alpha_pi = 1
beta_pi = 1
alpha_S = 1
beta_S = 1
alpha_C = 1
beta_C = 1

fGibbsSampling(20000, a, b)

```

# Problem 14.1.4
Suppose that a previous study that compares the clinical test with a laboratory gold standard concludes that S ∼ beta(10, 1) and C ∼ beta(10, 1). Use Gibbs sampling to estimate the new posterior for π. Why does this look different to your previously-estimated distribution?

```{r}
set.seed(41)
Y1 <- vector()
Y2 <- vector()
lpi <- vector()
S <- vector()
C <- vector()

a <- 20
b <- 80

Y1[1] <- as.integer(runif(1, 1, a))
  
Y2[1] <- as.integer(runif(1, a, b+a))
  
lpi[1] <- runif(1, 0, 1)
  
S[1] <- runif(1, 0, 1) 
  
C[1] <- runif(1, 0, 1) 

# Priors
alpha_pi = 1
beta_pi = 1
alpha_S = 10 # Updated prior
beta_S = 10
alpha_C = 10 # Updated prior
beta_C = 1

fGibbsSampling(20000, a, b)

```

#Problem 14.1.5
Suppose a previous analysis concluded that π ∼ beta(1, 10). Using this distribution as a prior, together with uniform priors on S and C, determine the posterior distributions for the test sensitivity and specificity respectively. Why does the test appear to be quite specific, although it is unclear how sensitive it is?

```{r}
set.seed(41)
Y1 <- vector()
Y2 <- vector()
lpi <- vector()
S <- vector()
C <- vector()

a <- 20
b <- 80

Y1[1] <- as.integer(runif(1, 1, a))
  
Y2[1] <- as.integer(runif(1, a, b+a))
  
lpi[1] <- runif(1, 0, 1)
  
S[1] <- runif(1, 0, 1)
  
C[1] <- runif(1, 0, 1)

# Priors
alpha_pi = 1
beta_pi = 10
alpha_S = 1
beta_S = 1
alpha_C = 1
beta_C = 1

fGibbsSampling(20000, a, b)
```

